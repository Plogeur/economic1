{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Plogeur/economic1/blob/main/ProjetML2_2022_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEfSujULiOkb"
      },
      "source": [
        "<H1> Les jeux de données pour le projet </H1>\n",
        "\n",
        "Dans ce notebook nous présentons les jeux de données utilisés pour le projet. Nous proposons également des fonctions pour permettre de pouvoir facilement les données.   \n",
        "\n",
        "Il n'y a donc plus qu'à chercher les meilleurs modèles et à répondre aux questions de l'énoncé du projet.   \n",
        "\n",
        "Bon courage !\n",
        "\n",
        "ps : il y a trois jeux de données et ils sont très différents donc attention vous aurez peut être 3 modèles différents. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQSC8frX5Ao"
      },
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zICeCUqYCfS"
      },
      "source": [
        "\n",
        "Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :  \n",
        "\n",
        "*! pip install nom_librairie*  \n",
        "\n",
        "**Attention :** il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n",
        "\n",
        "**Remarque :** même si toutes les librairies sont importées dès le début, les librairies utiles pour des fonctions présentées au cours de ce notebook sont ré-importées de manière à indiquer d'où elles viennent et ainsi faciliter la réutilisation de la fonction dans un autre projet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I0OpnDBHYHmv",
        "outputId": "3d0f04bf-a568-4566-f504-29ab90e1e1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn[plot]\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (0.56.3)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (3.2.2)\n",
            "Collecting datashader\n",
            "  Downloading datashader-0.14.2-py2.py3-none-any.whl (18.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (2.3.3)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.14.9)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (3.0.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (0.11.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (0.18.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn[plot]) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn[plot]) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn[plot]) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn[plot]) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn[plot]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn[plot]) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (5.1.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (7.1.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (21.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (6.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->umap-learn[plot]) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->umap-learn[plot]) (1.15.0)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from colorcet->umap-learn[plot]) (0.4.8)\n",
            "Requirement already satisfied: param>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyct>=0.4.4->colorcet->umap-learn[plot]) (1.12.2)\n",
            "Requirement already satisfied: xarray>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from datashader->umap-learn[plot]) (0.20.2)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (from datashader->umap-learn[plot]) (2022.2.0)\n",
            "Collecting datashape>=0.5.1\n",
            "  Downloading datashape-0.5.2.tar.gz (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from datashader->umap-learn[plot]) (2.23.0)\n",
            "Requirement already satisfied: multipledispatch>=0.4.7 in /usr/local/lib/python3.7/dist-packages (from datashape>=0.5.1->datashader->umap-learn[plot]) (0.6.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->umap-learn[plot]) (2022.5)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader->umap-learn[plot]) (2022.10.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader->umap-learn[plot]) (0.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader->umap-learn[plot]) (1.5.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader->umap-learn[plot]) (1.3.0)\n",
            "Requirement already satisfied: distributed==2022.02.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]->datashader->umap-learn[plot]) (2022.2.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (5.4.8)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (2.2.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (1.0.4)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask[complete]->datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed==2022.02.0->dask[complete]->datashader->umap-learn[plot]) (1.0.1)\n",
            "Requirement already satisfied: panel>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from holoviews->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from holoviews->umap-learn[plot]) (2.2.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews->umap-learn[plot]) (3.4.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews->umap-learn[plot]) (5.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->panel>=0.8.0->holoviews->umap-learn[plot]) (0.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->umap-learn[plot]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->umap-learn[plot]) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->datashader->umap-learn[plot]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->datashader->umap-learn[plot]) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->datashader->umap-learn[plot]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->datashader->umap-learn[plot]) (3.0.4)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (2021.11.2)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, datashape\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=886962c86e0d97ff777ce8716f8686abb8c47887b394fa317b946f7ccf98447b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55512 sha256=5871bb640243419b7ef8ac46ee032620a2dec179860d59cfba0221ec5eb41778\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/bc/eb/974072a56a7082a302f8b4be1ad6d21bf5019235c2eff65928\n",
            "  Building wheel for datashape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datashape: filename=datashape-0.5.2-py3-none-any.whl size=59438 sha256=c8c16893ac6323ba5fd6678b828759ace9a7451d7daf15c9fb7409db8c35441a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/80/333a5c3312ed4cd54f5d5b869868c14e0c6002cb5c7238b52d\n",
            "Successfully built umap-learn pynndescent datashape\n",
            "Installing collected packages: pynndescent, datashape, umap-learn, datashader\n",
            "Successfully installed datashader-0.14.2 datashape-0.5.2 pynndescent-0.5.8 umap-learn-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.7/dist-packages (1.14.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from holoviews) (21.3)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.7/dist-packages (from holoviews) (3.0.1)\n",
            "Requirement already satisfied: panel>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from holoviews) (0.12.1)\n",
            "Requirement already satisfied: param<2.0,>=1.9.3 in /usr/local/lib/python3.7/dist-packages (from holoviews) (1.12.2)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from holoviews) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from holoviews) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from holoviews) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->holoviews) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->holoviews) (2022.5)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (3.4.1)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (0.4.8)\n",
            "Requirement already satisfied: bokeh<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (2.3.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (5.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (4.64.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (5.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (4.1.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (2.11.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (6.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->holoviews) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.20.0->holoviews) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->panel>=0.8.0->holoviews) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown->panel>=0.8.0->holoviews) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->panel>=0.8.0->holoviews) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (5.3.4)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.16.2-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (6.1.12)\n",
            "Collecting matplotlib-inline>=0.1\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (23.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.4.8)\n",
            "Collecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting tornado>=6.1\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipykernel) (21.3)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.1.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (2.0.10)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel) (4.11.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipykernel) (3.0.9)\n",
            "Installing collected packages: tornado, matplotlib-inline, jedi, nest-asyncio, ipython, ipykernel\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.9.0\n",
            "    Uninstalling ipython-7.9.0:\n",
            "      Successfully uninstalled ipython-7.9.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.3.4\n",
            "    Uninstalling ipykernel-5.3.4:\n",
            "      Successfully uninstalled ipykernel-5.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel~=5.3.4, but you have ipykernel 6.16.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed ipykernel-6.16.2 ipython-7.34.0 jedi-0.18.1 matplotlib-inline-0.1.6 nest-asyncio-1.5.6 tornado-6.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# utiliser cette cellule pour installer les librairies manquantes\n",
        "# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n",
        "# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n",
        "# recommencer tant que toutes les librairies ne sont pas installées ...\n",
        "\n",
        "# sous Colab il faut déjà intégrer ces deux librairies\n",
        "\n",
        "!pip install umap-learn[plot]\n",
        "!pip install holoviews\n",
        "!pip install -U ipykernel\n",
        "\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "#indique des futurs warnings. \n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# librairies générales\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import shutil\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# TensorFlow et keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# librairies générales\n",
        "from tabulate import tabulate\n",
        "import time\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# librairies scikit learn\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# librairies des classifiers utilisés\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# librairies NLTK\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(12345)\n",
        "tf.random.set_seed(12345)"
      ],
      "metadata": {
        "id": "vGDDRV7upNPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n70qVaRWwII"
      },
      "outputs": [],
      "source": [
        "def plot_curves_confusion (history,confusion_matrix,class_names):\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1, top = 0.9, wspace = 0.25, hspace = 0)\n",
        "\n",
        "  # division de la fenêtre graphique en 1 ligne, 3 colonnes,\n",
        "  # graphique en position 1 - loss fonction\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "\n",
        "  # graphique en position 2 - accuracy\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "  \n",
        "  # matrice de correlation\n",
        "  plt.subplot(1,3,3)\n",
        "  #sns.heatmap(conf,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n",
        "  # labels, title and ticks\n",
        "  plt.xlabel('Predicted', fontsize=12)\n",
        "  #plt.set_label_position('top') \n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  #plt.tick_top()\n",
        "  plt.title(\"Correlation matrix\")\n",
        "  plt.ylabel('True', fontsize=12)\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  plt.show()\n",
        "\n",
        "def plot_curves(histories) :\n",
        "    plt.figure(1,figsize=(16,6))\n",
        "    plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "    for i in range(len(histories)) :\n",
        "    \t# plot loss\n",
        "        plt.subplot(121)\n",
        "        plt.title('Cross Entropy Loss')\n",
        "        plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "        plt.plot(histories[i].history['val_loss'], color='red', label='test')  \n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "    \t  # plot accuracy\n",
        "        plt.subplot(122)\n",
        "        plt.title('Classification Accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "        plt.plot(histories[i].history['val_accuracy'], color='red', label='test')\n",
        "    plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s2sF7a-am4I"
      },
      "source": [
        "Pour pouvoir sauvegarder sur votre répertoire Google Drive, il est nécessaire de fournir une autorisation. Pour cela il suffit d'éxecuter la ligne suivante et de saisir le code donné par Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "q1zULJ9daqP6",
        "outputId": "6ae4a1f4-635b-45e3-befd-0942c447b0fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-95d6df15acda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pour monter son drive Google Drive local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qe4Xtohau6v"
      },
      "source": [
        "Corriger éventuellement la ligne ci-dessous pour mettre le chemin vers un répertoire spécifique dans votre répertoire Google Drive : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahDl5RV9tVfp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "my_local_drive='/content/gdrive/My Drive/'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%mkdir Colab_Notebooks\n",
        "%cd Colab_Notebooks\n",
        "%mkdir HAI923\n",
        "%cd HAI923\n",
        "\n",
        "my_local_drive='/content/gdrive/My Drive/Colab_Notebooks/HAI923'\n",
        "\n",
        "sys.path.append(my_local_drive)\n",
        "\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDbIEQR-DPw"
      },
      "source": [
        "###Les jeux de données\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Ci4vc2mz0i"
      },
      "source": [
        "Récupération des jeux de données :      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTUWPaTYm4Tr"
      },
      "outputs": [],
      "source": [
        "!wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"Data_Project\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqC8-3wMZC1t"
      },
      "source": [
        "\n",
        "Il y a trois jeux de données différents : des tigres, des éléphants et des renards. Pour chacun d'entre eux il y a un ensemble d'images positive et un ensemble d'images négatives. Par exemple dans le répertoire *tiger* il n'y a que des images de tigre et dans le répertoire *Tiger_negative_class* il n'y a que des images d'animaux qui ne correspondent pas à des tigres.   \n",
        "\n",
        "Le code ci-dessous permet de visualiser quelques images contenues dans le répertoire *tiger*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVs5P1VmaAYr"
      },
      "outputs": [],
      "source": [
        "mypath='Data_Project/Tiger-Fox-Elephant/tiger'\n",
        "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
        "images = np.empty(len(onlyfiles), dtype=object)\n",
        "for n in range(0, len(onlyfiles)):\n",
        "  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )\n",
        "\n",
        "COLUMNS = 25 # Nombre d'images à afficher\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    # il faut donc convertir pour afficher les bonnes couleurs\n",
        "    images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(images[i],cmap=plt.cm.binary)  \n",
        "    plt.xlabel('taille ' + str(images[i].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eRP1h1fawL8"
      },
      "source": [
        "Nous pouvons constater que les images ne sont pas de la même taille. Il faut donc les convertir. Une manière simple de faire et de faire la conversion lors de la lecture des images : ici nous convertissons toutes les images en 124x124."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkR8CZEObaKx"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE=124\n",
        "mypath='Data_Project/Tiger-Fox-Elephant/tiger'\n",
        "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
        "images = np.empty(len(onlyfiles), dtype=object)\n",
        "for n in range(0, len(onlyfiles)):\n",
        "  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )\n",
        "  images[n]  = cv2.resize(images[n], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    # il faut donc convertir pour afficher les bonnes couleurs\n",
        "    images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(images[i],cmap=plt.cm.binary)  \n",
        "    plt.xlabel('taille ' + str(images[i].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix_SIuG9b9fn"
      },
      "source": [
        "**Créer le jeu de données**   \n",
        "\n",
        "Actuellement pour chaque animal nous avons un répertoire qui contient des images positives et un répertoire qui contient des images négatives. Pour pouvoir créer un jeu de données nous devons obtenir X et y. Les fonctions ci-dessous permettent de générer, à partir des répertoires, un jeu de données aléatoire pour X et y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02QZLNadp9A"
      },
      "outputs": [],
      "source": [
        "def create_training_data(path_data, list_classes):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "      path=os.path.join(path_data, classes)\n",
        "      class_num=list_classes.index(classes)\n",
        "      for img in os.listdir(path):\n",
        "        try :\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_UNCHANGED)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          training_data.append([new_array, class_num])\n",
        "        except Exception as e:\n",
        "          pass  \n",
        "  return training_data    \n",
        "\n",
        "def create_X_y (path_data, list_classes):\n",
        "      # récupération des données\n",
        "      training_data=create_training_data(path_data, list_classes)\n",
        "      # tri des données\n",
        "      random.shuffle(training_data)\n",
        "      # création de X et y\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      y=np.array(y)\n",
        "      return X,y\n",
        "\n",
        "def plot_examples(X,y):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    X[i] = cv2.cvtColor(X[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(X[i]/255.,cmap=plt.cm.binary)\n",
        "    plt.xlabel('classe ' + str(y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smgGc_PbeDt_"
      },
      "source": [
        "Définition de constante globale      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK1qXTdyeIEF"
      },
      "outputs": [],
      "source": [
        "# constantes globales\n",
        "IMG_SIZE = 124 # Taille de l'image\n",
        "COLUMNS = 25 # Nombre d'images à afficher\n",
        "EPOCHS = 100 # Nombre d'epoch\n",
        "BATCH_SIZE = 15 # Pour le traitement par lot des images (optimisation de la dencité de gradient)\n",
        "STOPPING_PATIENCE = 10 # Callback pour stopper si le modèle n'apprend plus\n",
        "VERBOSE = 1 # Barre de chargement (0 : off, 1 : on, 2 : on mais moche)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YabKaKpz9yhJ"
      },
      "source": [
        "# Image generator (increase the number of picture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srfjqm6e9yQ2"
      },
      "outputs": [],
      "source": [
        "#Run just ONE time\n",
        "if os.path.exists('Data_Project/Tiger-Fox-Elephant/tiger/modify_0_9.jpg') :\n",
        "  pass\n",
        "else : \n",
        "  datagen = ImageDataGenerator(\n",
        "          rotation_range=20,\n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1,\n",
        "          shear_range=0.1,\n",
        "          zoom_range=0.1,\n",
        "          horizontal_flip=True,\n",
        "          fill_mode='nearest')\n",
        "\n",
        "  main_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "\n",
        "  #iterate over files in that directory\n",
        "  for rep in os.listdir(main_path) :\n",
        "    if rep == \".DS_Store\" :\n",
        "      continue\n",
        "    for filename in os.listdir(main_path + rep) :\n",
        "      f = os.path.join(main_path + rep, filename)\n",
        "      img = load_img(f)  # this is a PIL image\n",
        "      x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
        "      x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
        "      i = 0\n",
        "\n",
        "      for batch in datagen.flow(x, save_to_dir=main_path + rep, save_prefix='modify', batch_size=1, save_format='jpg') :\n",
        "        i += 1\n",
        "        if i > 15 :\n",
        "          break  # otherwise the generator would loop indefinitely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2daZYyk9gYAM"
      },
      "source": [
        "Pour les éléphants :     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZIV7xlzgbYq"
      },
      "outputs": [],
      "source": [
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['elephant','Elephant_negative_class']\n",
        "X,y=create_X_y (my_path,my_classes)\n",
        "print (\"Nombre de données : \",X.shape[0])\n",
        "print (\"Taille d'une image pour connaître l'input du réseau\", X[0].shape)\n",
        "print (\"Distribution des labels dans le jeu d'apprentissage\")\n",
        "sns.countplot(np.array(y))\n",
        "plt.title(\"Nombre d'éléments par classe\")\n",
        "# affichage\n",
        "plot_examples(X,y)\n",
        "\n",
        "# Surtout ne pas oublier de normaliser les données avec :\n",
        "X=X.astype('float')\n",
        "X=X/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bIEGTWgt8q"
      },
      "source": [
        "Pour les renards :     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COgu0tUWgxUL"
      },
      "outputs": [],
      "source": [
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['fox','Fox_negative_class']\n",
        "X,y=create_X_y (my_path,my_classes)\n",
        "print (\"Nombre de données : \",X.shape[0])\n",
        "print (\"Taille d'une image pour connaître l'input du réseau\", X[0].shape)\n",
        "print (\"Distribution des labels dans le jeu d'apprentissage\")\n",
        "sns.countplot(np.array(y))\n",
        "plt.title(\"Nombre d'éléments par classe\")\n",
        "# affichage\n",
        "plot_examples(X,y)\n",
        "\n",
        "# Surtout ne pas oublier de normaliser les données avec :\n",
        "X=X.astype('float')\n",
        "X=X/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmo8-X4VeyyL"
      },
      "source": [
        "Pour les tigres :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgIHX8KWemlj"
      },
      "outputs": [],
      "source": [
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "X,y=create_X_y (my_path,my_classes)\n",
        "print (\"Nombre de données : \",X.shape[0])\n",
        "print (\"Taille d'une image pour connaître l'input du réseau\", X[0].shape)\n",
        "print (\"Distribution des labels dans le jeu d'apprentissage\")\n",
        "sns.countplot(np.array(y))\n",
        "plt.title(\"Nombre d'éléments par classe\")\n",
        "# affichage\n",
        "plot_examples(X,y)\n",
        "\n",
        "# Surtout ne pas oublier de normaliser les données avec :\n",
        "X=X.astype('float')\n",
        "X=X/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4YUILAuC1JZ"
      },
      "source": [
        "# Slip dataset : train, test, validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kjq7lxnP5k8"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "#print len\n",
        "print(\"length of X_train : \", len(X_train), \", length of y_train : \", len(y_train))\n",
        "print(\"length of X_test : \", len(X_test), \", length of y_test : \", len(y_test))\n",
        "\n",
        "# one hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkNuTsESKtHM"
      },
      "source": [
        "#Callbacks Keras (optimisation training step) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Ibpgz3BaxL"
      },
      "outputs": [],
      "source": [
        "# Permet de stopper l'apprentissage si il stagne\n",
        "EARLY_STOPPING = \\\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=STOPPING_PATIENCE,\n",
        "            verbose=VERBOSE,\n",
        "            mode='auto')\n",
        "\n",
        "# Reduit le LearningRate si stagnation\n",
        "LR_REDUCTION = \\\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            patience=5,\n",
        "            verbose=VERBOSE,\n",
        "            factor=0.5,\n",
        "            min_lr=0.00001)\n",
        "\n",
        "CALLBACKS = [EARLY_STOPPING, LR_REDUCTION]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boLNYGRA3Aso"
      },
      "source": [
        "#Création du model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvD3zA4LeSK6"
      },
      "source": [
        "Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTtestswLBq9"
      },
      "outputs": [],
      "source": [
        "cnnmodel = Sequential()\n",
        "# 1 couche de convolution, avec nombre de filtres progressif 32\n",
        "cnnmodel.add(Conv2D(filters=32, kernel_size=(3,3), input_shape=(124, 124, 3), activation='relu'))\n",
        "cnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# remise à plat\n",
        "cnnmodel.add(Flatten())\n",
        "# Couche dense classique ANN\n",
        "cnnmodel.add(Dense(100, activation='relu')) #Examiner les modifications effectuées à distance\n",
        "\n",
        "# Couche de sortie : classification => softmax sur le nombre de classe\n",
        "cnnmodel.add(Dense(units=2, activation='softmax', name='softmax'))\n",
        "\n",
        "opt = SGD(learning_rate=0.01)\n",
        "cnnmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Affichage du résumer\n",
        "cnnmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0euas-CeUHZ"
      },
      "source": [
        "  Advance model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OW7I7xToxGr"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Réalisation des couches de Convolution  / Pooling\n",
        "# ---- Conv / Pool N°1\n",
        "model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same', input_shape=(124, 124, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "# ---- Conv / Pool N°2\n",
        "model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "# ---- Conv / Pool N°3\n",
        "model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "# ---- Conv / Pool N°4\n",
        "model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "# Flattening : passage de matrices 3D vers un vecteur\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Couche de sortie : classification => softmax sur le nombre de classe\n",
        "model.add(Dense(units=2, activation='softmax', name='softmax'))\n",
        "\n",
        "# compilation du  model de classification\n",
        "opt = SGD(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Affichage du résumer\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7zfQJAl3K9F"
      },
      "source": [
        "#Affichage des résultats de l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AQWNQEXPG6j"
      },
      "outputs": [],
      "source": [
        "def affichage_plot(histories, scores) :\n",
        "  # plot diagnostic learning curves \n",
        "  for i in range(len(histories)) :\n",
        "  # plot accuracy\n",
        "    plt.figure(1, figsize = (15,8))\n",
        "    plt.subplot(221)  \n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(histories[i].history['val_accuracy'], color='red', label='test')\n",
        "    #plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "    # plot loss\n",
        "    plt.subplot(222)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    plt.plot(histories[i].history['val_loss'], color='red', label='test')\n",
        "    #plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  # plot score\n",
        "  if len(scores) > 1 :\n",
        "    plt.subplot(312)\n",
        "    plt.title('Classification Score')\n",
        "    plt.plot([n for n in range(len(scores))], scores, color='green', label='score')\n",
        "    plt.ylabel('Cross-Validated-Accuracy')\n",
        "    plt.xlabel('Value of K')\n",
        "    plt.legend(['score'], loc='upper left') \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRjyqHDU3F2B"
      },
      "source": [
        "#Entrainement du model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eqcE9LsGETF"
      },
      "source": [
        "Entrainement du baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "E7RvMGhife71",
        "outputId": "7696a49e-de3b-4a9d-c70b-0e57a1bfd64b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e78ecf8a40dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_cnnmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCALLBACKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodel_training_apis\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstackoverflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m49922252\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mchoosing\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   fit(object, x = NULL, y = NULL, batch_size = NULL, epochs = 10,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "history_cnnmodel = model.fit(X_train, y_train, epochs=EPOCHS, callbacks=CALLBACKS, batch_size=24, validation_data=(X_test, y_test), verbose=1)\n",
        "\"\"\"\n",
        "https://keras.io/api/models/model_training_apis/\n",
        "https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
        "  fit(object, x = NULL, y = NULL, batch_size = NULL, epochs = 10,\n",
        "  verbose = getOption(\"keras.fit_verbose\", default = 1),\n",
        "  callbacks = NULL, view_metrics = getOption(\"keras.view_metrics\",\n",
        "  default = \"auto\"), validation_split = 0, validation_data = NULL,\n",
        "  shuffle = TRUE, class_weight = NULL, sample_weight = NULL,\n",
        "  initial_epoch = 0, steps_per_epoch = NULL, validation_steps = NULL,\n",
        "  ...)\n",
        "\"\"\"\n",
        "histories_cnnmodel = [history_cnnmodel]\n",
        "score = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiIN7wQLQ8jf"
      },
      "outputs": [],
      "source": [
        "#Affigage\n",
        "affichage_plot(histories_cnnmodel, score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stY2e9CzREGk"
      },
      "outputs": [],
      "source": [
        "#Evaluation du model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
        "\n",
        "#print len\n",
        "print(\"length of X_train : \", len(X_train), \", length of y_train : \", len(y_train))\n",
        "#print(\"length of X_val : \", len(X_val), \", length of y_val : \", len(y_val))\n",
        "print(\"length of X_test : \", len(X_test), \", length of y_test : \", len(y_test))\n",
        "\n",
        "# one hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "#y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "results_cnnmodel = cnnmodel.evaluate(X_test, y_test, batch_size=10)\n",
        "print(\"test loss, test acc : \", results_cnnmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjuWS5WTGBUo"
      },
      "source": [
        "Entrainement du modèle avancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oixgVUv_EFcE"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataX, dataY, n_folds=5) :\n",
        "  scores, histories = list(), list()\n",
        "  kfold = KFold(n_folds = 10, shuffle=True, random_state=1)\n",
        "  # parcourir les splits du k-fold\n",
        "  #A REVOIR SUPPRESSION DE L'APPRENTISSAGE ENTRE LES K-FOLD et suppression du score\n",
        "  for train_ix, val_ix in kfold.split(dataX):\n",
        "    # selection des données\n",
        "    X_train, y_train, X_val, y_val = dataX[train_ix], dataY[train_ix], dataX[val_ix], dataY[val_ix]\n",
        "    # fit du modele\n",
        "    history = model.fit(X_train, y_train, epochs=EPOCHS, callbacks=CALLBACKS, batch_size=50, validation_data=(X_val, y_val), verbose=1)\n",
        "    # evaluate du modele\n",
        "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print('accuracy %.3f' % (acc * 100.0))\n",
        "    scores.append(acc)\n",
        "    histories.append(history)\n",
        "  return scores, histories"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross val\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=12345)\n",
        "results = cross_val_score(model, X, y, cv=kfold)\n",
        "\n",
        "plt.subplot(221)  \n",
        "plt.title('Classification Score')\n",
        "plt.plot(list(range(1,10)), results)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Cross-Validated-Accuracy')"
      ],
      "metadata": {
        "id": "t7AZ965RIkoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy2er8o6F8ma"
      },
      "outputs": [],
      "source": [
        "#Entrainement K-fold\n",
        "scores, history_advance = evaluate_model(model, X_train, y_train, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4g8h_YM6k6q"
      },
      "outputs": [],
      "source": [
        "#Affigage\n",
        "affichage_plot(history_advance, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNXmRZnoRQaa"
      },
      "outputs": [],
      "source": [
        "#Evaluation du model\n",
        "results_advance = model.evaluate(X_test, y_test, batch_size=10)\n",
        "print(\"test loss, test acc : \", results_advance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td5tGHliG6Lf"
      },
      "source": [
        "#Transfert learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def affiche(histroy) :\n",
        "  plt.figure(1, figsize = (15,8)) \n",
        "  plt.subplot(221)  \n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "  plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "  # plot loss\n",
        "  plt.subplot(222)\n",
        "  plt.title('Cross Entropy Loss')\n",
        "  plt.plot(history.history['loss'], color='blue', label='train')\n",
        "  plt.plot(history.history['val_loss'], color='red', label='test')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "clbCKGdGRAbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (124, 124, 3)\n",
        "resnet_model = Sequential()\n",
        "\n",
        "pretrained_model= tf.keras.applications.ResNet50(include_top=False, input_shape=IMG_SHAPE, pooling='avg',classes=2, weights='imagenet')\n",
        "\n",
        "for layer in pretrained_model.layers:\n",
        "        layer.trainable=False\n",
        "\n",
        "resnet_model.add(pretrained_model)\n",
        "resnet_model.add(Flatten())\n",
        "resnet_model.add(Dense(512, activation='relu'))\n",
        "resnet_model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "resnet_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "kdZ_W7hzX44v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = resnet_model.fit(X_train, y_train, epochs=50, callbacks=CALLBACKS, batch_size=10, steps_per_epoch=len(X_train)//20, validation_data=(X_test, y_test))\n",
        "affiche(history)"
      ],
      "metadata": {
        "id": "iqIL3G19Q4NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (124, 124, 3)\n",
        "resnet_model = Sequential()\n",
        "\n",
        "pretrained_model= tf.keras.applications.ResNet50(include_top=False, input_shape=IMG_SHAPE, pooling='avg',classes=2, weights='imagenet')\n",
        "\n",
        "for layer in pretrained_model.layers:\n",
        "        layer.trainable=False\n",
        "\n",
        "resnet_model.add(pretrained_model)\n",
        "resnet_model.add(Flatten())\n",
        "resnet_model.add(Dense(512, activation='relu'))\n",
        "resnet_model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "resnet_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history2 = resnet_model.fit(X_train, y_train, epochs=50, callbacks=CALLBACKS, batch_size=25, steps_per_epoch=len(X_train)//20, validation_data=(X_test, y_test))\n",
        "affiche(history2)"
      ],
      "metadata": {
        "id": "p-umpoZvRkkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (124, 124, 3)\n",
        "resnet_model = Sequential()\n",
        "\n",
        "pretrained_model= tf.keras.applications.ResNet50(include_top=False, input_shape=IMG_SHAPE, pooling='avg',classes=2, weights='imagenet')\n",
        "\n",
        "for layer in pretrained_model.layers:\n",
        "        layer.trainable=False\n",
        "\n",
        "resnet_model.add(pretrained_model)\n",
        "resnet_model.add(Flatten())\n",
        "resnet_model.add(Dense(512, activation='relu'))\n",
        "resnet_model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "resnet_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history3 = resnet_model.fit(X_train, y_train, epochs=50, callbacks=CALLBACKS, batch_size=100, steps_per_epoch=len(X_train)//20, validation_data=(X_test, y_test))\n",
        "affiche(history3)"
      ],
      "metadata": {
        "id": "K_WFmGaeRk_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRV1m9jj5iEC"
      },
      "source": [
        "#Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyqkPD2D5fpe"
      },
      "outputs": [],
      "source": [
        "#save the model\n",
        "model.save('Data_Project/model')\n",
        "\n",
        "#It can be used to reconstruct the model identically.\n",
        "#reconstructed_model = tf.keras.models.load_model('Data_Project/model')\n",
        "#or\n",
        "#reconstructed_model = keras.models.load_model('Data_Project/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm3peA1x9RDQ"
      },
      "outputs": [],
      "source": [
        "# TODO :\n",
        "# - Revoir le k-fold (désapprendre le model pour chaque tour de boucle k-fold)\n",
        "# - trouver les meilleur hyperparametre pour les models :\n",
        "#   https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac\n",
        "# - commenter le code + texte\n",
        "# - GAN\n",
        "# - Afficher le contenu des sorties des CNN"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}